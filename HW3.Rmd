---
title: "HW3"
author: "Ben Hertzberg"
date: '2022-10-26'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
titanic <- read.csv('./data/titanic.csv')
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
tidymodels_prefer()

```


```{r}
titanic$survived <- as.factor(titanic$survived)
titanic$pclass <- as.factor(titanic$pclass)
```


### Question 1

```{r}
set.seed(619)

ttnc_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)
ttnc_train <- training(ttnc_split)
ttnc_test <- testing(ttnc_split)
```

```{r}
ttnc_train
```

There is some data missing for the age and cabin variables. Some of the data for the ticket variable seems to be formatted incorrectly.

It is a good idea to use stratified sampling for this data because the model should be exposed to a reasonable number of observations for every possibility of 'survived'. Without stratification, it is possible that one possibility of the outcome of 'survive' will be overly sampled, which could confuse the model.

### Question 2


```{r}
ttnc_train %>%
  ggplot(aes(x = survived)) +
  geom_bar()
```

The outcome variable, survived, has more 'no' observations than 'yes' observations. However, there is a large number of both possibilities present, so the data should be fine for training.

### Question 3


```{r}
cor_ttnc <- ttnc_train %>%
  dplyr::select(age, sib_sp, parch, fare) %>%
  correlate()
rplot(cor_ttnc)

```

There is a slight negative correlation between age and sib_sp, and a slight positive correlation between sib_sp and parch.

### Question 4

```{r}
ttnc_rec <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = ttnc_train) %>%

  step_impute_linear() %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_normalize(all_numeric_predictors()) %>%
  step_interact(terms = ~starts_with("sex"):fare) %>%
  step_interact(terms = ~age:fare)
 
 
```


### Question 5

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_wkflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(ttnc_rec)

log_fit <- fit(log_wkflow, ttnc_train)
```

### Question 6

```{r}
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")

lda_wkflow <- workflow() %>%
  add_model(lda_mod) %>%
  add_recipe(ttnc_rec)

lda_fit <- fit(lda_wkflow, ttnc_train)
```

### Question 7

```{r}
qda_mod <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_wkflow <- workflow() %>%
  add_model(qda_mod) %>%
  add_recipe(ttnc_rec)

qda_fit <- fit(qda_wkflow, ttnc_train)
```


### Question 8

```{r}
nb_mod <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR") %>%
  set_args(usekernel = FALSE)

nb_wkflow <- workflow() %>%
  add_model(nb_mod) %>%
  add_recipe(ttnc_rec)

nb_fit <- fit(nb_wkflow, ttnc_train)


#Received error during fit:
# Error in if (any(temp)) stop("Zero variances for at least one class in variables: ", : 
# missing value where TRUE/FALSE needed

#Tried to Google, reach out to friends, but could not figure it out. Tried to rearrange steps in the recipe, changed variable types, and use different syntax for the naive bayes model.
```

```{r}
log_fit %>%
  tidy()
```

### Question 9

Now you've fit four different models to your training data.

Use `predict()` and `bind_cols()` to generate predictions using each of these 4 models and your **training** data. Then use the *accuracy* metric to assess the performance of each of the four models.

```{r, warning=FALSE}
predict(log_fit, new_data = ttnc_train, type = 'prob')
predict(lda_fit, new_data = ttnc_train, type = 'prob')
predict(qda_fit, new_data = ttnc_train, type = 'prob')
#predict(nb_fit, new_data = ttnc_train, type = 'prob')
```



```{r, warning=FALSE}
log_reg_acc <- augment(log_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
log_reg_acc

lda_acc <- augment(lda_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
lda_acc

qda_acc <- augment(qda_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
qda_acc

#nb_acc <- augment(nb_fit, new_data = ttnc_train) %>%
#  accuracy(truth = factor(survived), estimate = .pred_class)
#nb_acc


```


The logorithmic regression model achieved the highest accuracy on the training data.


### Question 10

```{r}
log_reg_test_acc <- augment(log_fit, new_data = ttnc_test) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
log_reg_test_acc
```

The log reg model's accuracy on the training data is 75.52%

```{r}
augment(log_fit, new_data = ttnc_test) %>%
  conf_mat(truth = survived, estimate = .pred_class) 

augment(log_fit, new_data = ttnc_test) %>%
  roc_curve(survived, .pred_No) %>%
  autoplot()

log_results <- augment(log_fit, new_data = ttnc_test)

log_results %>% 
  roc_auc(survived, .pred_No)
```

The model performed quite well. The training accuracy was 80.91%. The testing accuracy was slightly lower at 75.52%, but that is to be expected when fitting the testing data. This is because the model specializes on the training data that it learns from, so it will naturally perform better on the training data.