---
title: "HW3"
author: "Ben Hertzberg"
date: '2022-10-26'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
titanic <- read.csv('./data/titanic.csv')
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
tidymodels_prefer()

```


Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

```{r}
titanic$survived <- as.factor(titanic$survived)
titanic$pclass <- as.factor(titanic$pclass)
```


*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

### Question 1

```{r}
set.seed(619)

ttnc_split <- initial_split(titanic, prop = 0.80,
                                strata = survived)
ttnc_train <- training(ttnc_split)
ttnc_test <- testing(ttnc_split)
```

```{r}
ttnc_train
```

There is some data missing for the age and cabin variables. Some of the data for the ticket variable seems to be formatted incorrectly.

It is a good idea to use stratified sampling for this data because the model should be exposed to a reasonable number of observations for every possibility of 'survived'. Without stratification, it is possible that one possibility of the outcome of 'survive' will be overly sampled, which could confuse the model.

### Question 2

Using the **training** data set, explore/describe the distribution of the outcome variable `survived`.

```{r}
ttnc_train %>%
  ggplot(aes(x = survived)) +
  geom_bar()
```

The outcome variable, survived, has more 'no' observations than 'yes' observations. However, there is a large number of both possibilities present, so the data should be fine for training.

### Question 3

Using the **training** data set, create a correlation matrix of all continuous variables. Create a visualization of the matrix, and describe any patterns you see. Are any predictors correlated with each other? Which ones, and in which direction?

```{r}
cor_ttnc <- ttnc_train %>%
  dplyr::select(age, sib_sp, parch, fare) %>%
  correlate()
rplot(cor_ttnc)

```

There is a slight negative correlation between age and sib_sp, and a slight positive correlation between sib_sp and parch.

### Question 4

Using the **training** data, create a recipe predicting the outcome variable `survived`. Include the following predictors: ticket class, sex, age, number of siblings or spouses aboard, number of parents or children aboard, and passenger fare.

```{r}
ttnc_rec <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = ttnc_train) %>%

  step_impute_linear() %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_normalize(all_numeric_predictors()) %>%
  step_interact(terms = ~starts_with("sex"):fare) %>%
  step_interact(terms = ~age:fare)
 
 
```


### Question 5

```{r}
log_reg <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

log_wkflow <- workflow() %>%
  add_model(log_reg) %>%
  add_recipe(ttnc_rec)

log_fit <- fit(log_wkflow, ttnc_train)
```

### Question 6

```{r}
lda_mod <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")

lda_wkflow <- workflow() %>%
  add_model(lda_mod) %>%
  add_recipe(ttnc_rec)

lda_fit <- fit(lda_wkflow, ttnc_train)
```

### Question 7

```{r}
qda_mod <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_wkflow <- workflow() %>%
  add_model(qda_mod) %>%
  add_recipe(ttnc_rec)

qda_fit <- fit(qda_wkflow, ttnc_train)
```


### Question 8

**Repeat Question 5**, but this time specify a naive Bayes model for classification using the `"klaR"` engine. Set the `usekernel` argument to `FALSE`.

```{r}
nb_mod <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("klaR") %>%
  set_args(usekernel = FALSE)

nb_wkflow <- workflow() %>%
  add_model(nb_mod) %>%
  add_recipe(ttnc_rec)

nb_fit <- fit(nb_wkflow, ttnc_train)

```

```{r}
log_fit %>%
  tidy()
```

### Question 9

Now you've fit four different models to your training data.

Use `predict()` and `bind_cols()` to generate predictions using each of these 4 models and your **training** data. Then use the *accuracy* metric to assess the performance of each of the four models.

```{r, warning=FALSE}
predict(log_fit, new_data = ttnc_train, type = 'prob')
predict(lda_fit, new_data = ttnc_train, type = 'prob')
predict(qda_fit, new_data = ttnc_train, type = 'prob')
```



```{r, warning=FALSE}
log_reg_acc <- augment(log_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
log_reg_acc

lda_acc <- augment(lda_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
lda_acc

qda_acc <- augment(qda_fit, new_data = ttnc_train) %>%
  accuracy(truth = factor(survived), estimate = .pred_class)
qda_acc


```


Which model achieved the highest accuracy on the training data?



### Question 10

Fit the model with the highest training accuracy to the **testing** data. Report the accuracy of the model on the **testing** data.

Again using the **testing** data, create a confusion matrix and visualize it. Plot an ROC curve and calculate the area under it (AUC).

How did the model perform? Compare its training and testing accuracies. If the values differ, why do you think this is so?
