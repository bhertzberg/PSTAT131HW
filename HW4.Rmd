---
title: "HW4"
author: "Ben Hertzberg"
date: "2022-11-03"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
titanic <- read.csv('./data/titanic.csv')
library(tidymodels)
library(tidyverse)
library(ggplot2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
tidymodels_prefer()
```


```{r}
titanic$survived <- as.factor(titanic$survived)
titanic$pclass <- as.factor(titanic$pclass)
```


### Question 1

```{r}
set.seed(619)

ttnc_split <- initial_split(titanic, prop = 0.80, strata = survived)
ttnc_train <- training(ttnc_split)
ttnc_test <- testing(ttnc_split)

```

```{r}
dim(ttnc_train)
dim(ttnc_test)
```

712 training observations, 179 testing observations, which is about 80% training and 20% testing.

```{r}
ttnc_rec <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, data = ttnc_train) %>%

  step_impute_linear() %>%
  step_dummy(all_nominal_predictors()) %>%  
  step_normalize(all_numeric_predictors()) %>%
  step_interact(terms = ~starts_with("sex"):fare) %>%
  step_interact(terms = ~age:fare)
```

### Question 2

```{r}
ttnc_folds <- vfold_cv(ttnc_train, v = 10)
ttnc_folds
```


### Question 3

In Q2, we randomly assigned observations from the training data into 10 roughly equal subsets to be used in k-fold cross-validation. K-fold CV offers a way to check how well our model is doing before applying it on the testing data. This is done by training a model on every fold except one, and then effectively testing on that last fold. K total models are trained, each one using a different fold to test on and training on all the others. This offers the benefit of checking how a model works on data it was not trained on before actually applying it to the test data. If we fit and train models on the entire training set right away, we have to use the testing data to do this check, and then we can't make adjustments to the model without risking data leakage. If we resampled from the entire training set, we would be using bootstrapping.

### Question 4

```{r}
log_reg <- logistic_reg() %>% 
  set_engine('glm') %>% 
  set_mode('classification')

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(ttnc_rec)

lda_mod <- discrim_linear() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')

lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(ttnc_rec)

qda_mod <-discrim_quad() %>% 
  set_engine('MASS') %>% 
  set_mode('classification')

qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(ttnc_rec)
```

I am fitting 3 models to 10 folds, which leads to a total of 30 models.

### Question 5

```{r, eval = FALSE}
log_fit_k <- fit_resamples(log_wkflow, ttnc_folds)
lda_fit_k <- fit_resamples(lda_wkflow, ttnc_folds)
qda_fit_k <- fit_resamples(qda_wkflow, ttnc_folds)
```

```{r, eval = FALSE}
write_rds(log_fit_k, file = 'SavedModels/log_res.rds')
write_rds(lda_fit_k, file = 'SavedModels/lda_res.rds')
write_rds(qda_fit_k, file = 'SavedModels/qda_res.rds')
```

```{r}
log_res <- read_rds(file = 'SavedModels/log_res.rds')
lda_res <- read_rds(file = 'SavedModels/lda_res.rds')
qda_res <- read_rds(file = 'SavedModels/qda_res.rds')
```


### Question 6

```{r}
collect_metrics(log_res)
collect_metrics(lda_res)
collect_metrics(qda_res)
```

QDA performed the worst. LDA and Logistic Regression performed very similarly, but logistic regression has a slightly higher mean accuracy. The standard error of the two is almost identical, but the LDA's is barely lower. The difference in mean accuracy was larger than the difference in standard error, so I believe the logistic regression model performed the best.

### Question 7

```{r}
log_fit_final <- fit(log_wkflow, ttnc_train)
```

### Question 8

```{r}
predict(log_fit_final, new_data = ttnc_test, type = 'prob')

log_acc <- augment(log_fit_final, new_data = ttnc_test) %>% 
  accuracy(truth = survived, estimate = .pred_class)
log_acc
```

The LDA model test accuracy is 75.52%, lower than the average across folds which was 79.97%. The accuracy of a model on new data often is slightly lower, so these values are reasonable.
