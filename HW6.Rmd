---
title: "HW6"
author: "Ben Hertzberg"
date: "2022-11-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(corrplot)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
```

### Exercise 1

```{r}
pok <- read.csv('./data/Pokemon.csv')
pok <- clean_names(pok)
pok2 <- subset(pok, type_1 == c('Bug','Fire','Grass',"Normal",'Water','Psychic'))
pok2$type_1 <- as.factor(pok2$type_1)
pok2$legendary <- as.factor(pok2$legendary)
```

```{r}
set.seed(619)

pok_split <- initial_split(pok2, prop = 0.80, strata = type_1)
pok_train <- training(pok_split)
pok_test <- testing(pok_split)
dim(pok_train)
dim(pok_test)
```

```{r}
pok_folds <- vfold_cv(pok_train, v = 5, strata = type_1)
pok_folds
```

```{r}
pok_rec <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pok_train) %>% 
  
  step_dummy(legendary, generation) %>% 
  step_normalize(all_numeric_predictors())
```

### Exercise 2


```{r}
pok_train %>% 
  select(is.numeric, -x) %>% 
  cor(use = 'complete.obs') %>% 
  corrplot(type = 'lower', diag = FALSE)

```
I removed the variable called 'x' from the correlation matrix. This variable contains an ID number for each pokemon, and it is not relevant in predictions.

The variable 'total' has a strong positive correlation with every variable except generation. This makes sense because total is a sum of all these other values. Defense and attack are positively correletated, which migh mean that stronger pokemon perform better in both of these areas. Defense speed (sp_def) and attack speed (sp_atk) are also positively correlated; if a pokemon is fast in one area it makes sense they would be fast in the other.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

```{r}
tree_spec <- decision_tree() %>% 
  set_engine('rpart')

class_tree_spec <- tree_spec %>% 
  set_mode('classification')

class_tree_wf <- workflow() %>% 
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>% 
  add_recipe(pok_rec)
```

```{r, eval = FALSE}
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf,
  resamples = pok_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

write_rds(tune_res, file = 'SavedModels/poketree_tuned_res.rds')
                           
```


```{r}
poketree_tune_res <- read_rds(file = 'SavedModels/poketree_tuned_res.rds')

autoplot(poketree_tune_res)
```

The results for roc_auc are nearly constant until the cost complexity reached about 0.06, where the roc_auc slightly increased. As cost complexity increased beyond that value, roc_auc significantly decreased. In general, it seems like the single decision tree performed better with a smaller cost complexity penalty.

### Exercise 4

```{r}
collect_metrics(poketree_tune_res) %>% 
  arrange(desc(mean))

best_mod <- select_best(poketree_tune_res, metric = 'roc_auc')
```

The `roc_auc` of my best-performing pruned decision tree on the folds is 0.560

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
pok_final <- finalize_workflow(class_tree_wf, best_mod)
pok_final_fit <- fit(pok_final, data = pok_train)
pok_final_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()
```

### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.
```{r}
bagging_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>% 
  set_engine('ranger', importance = 'impurity') %>% 
  set_mode('classification')


rf_wf <- workflow() %>% 
  add_model(bagging_spec) %>% 
  add_recipe(pok_rec)
```

mtry specifies how many randomly chosen variables are available to the model at each split, which guarantees that not all trees make the same splits. trees is a value that specifies the number of trees the entire ensemble can contain. min_n specifies a minimum number of data points in a node to allow another split to be made.


Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
reg_grid <- grid_regular(mtry(range = c(1,8)), trees(range = c(10, 300)), min_n(range = c(2,12)), levels = 8)
```

You can't choose a mtry value less than 1 because then the model would have no variables to choose from to make a split, so no splits would be made. In thise case, you can't have a mtry value more than 8, because there are only 8 predictor variables, so you can't provide the model with more options to make a split on. A mtry value of 8 is bagging.

### Exercise 6


```{r, eval=FALSE}
tune_res2 <- tune_grid(
  rf_wf,
  resamples = pok_folds,
  grid = reg_grid
)

write_rds(tune_res2, file = 'SavedModels/poke_rf_tuned_res.rds')
```

```{r}
poke_rf_res <- read_rds(file = 'SavedModels/poke_rf_tuned_res.rds')

autoplot(poke_rf_res)
```
Models with fewer trees (less than 100) seemed to have a lot more variability. For every model, the number of randomly selected predictors seemed to have the largest effect on roc_auc.

Models with 2 to 4 randomly selected predictors performed the best. Of these, models with less than 100 trees had the highest peaks in roc_auc, but have far more variability than those with more trees.

### Exercise 7

```{r}
collect_metrics(poke_rf_res) %>% 
  arrange(desc(mean))

best_rf_mod <- select_best(poke_rf_res, metric = 'roc_auc')
```

The `roc_auc` of the best-performing random forest model is 0.6259

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.
```{r}
pok_rf_final <- finalize_workflow(rf_wf, best_rf_mod)
rf_fit <- fit(pok_rf_final, data = pok_train)
rf_fit
```

```{r}
vip(rf_fit)
```


Which variables were most useful? Which were least useful? Are these results what you expected, or not?

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

```{r}
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wf <- workflow() %>% 
  add_model(boost_spec) %>% 
  add_recipe(pok_rec)
```

```{r}
boost_grid <- grid_regular(trees(range = c(10, 2000)), levels = 10)

tune_res <- tune_grid(
  boost_wf,
  resamples = pok_folds,
  grid = boost_grid,
  metrics = metric_set(roc_auc)
)

write_rds(tune_res, file = 'SavedModels/poketree_boost_res.rds')
```

```{r}

poke_boost_res <- read_rds(file = 'SavedModels/poketree_boost_res.rds')

autoplot(poke_boost_res)
```

The roc_auc peaks around 500 trees at 0.5925. Increasing or deacreasing the trees leads to a large drop in roc_auc.

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

## For 231 Students

### Exercise 11

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?