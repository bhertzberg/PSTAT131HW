---
title: "HW6"
author: "Ben Hertzberg"
date: "2022-11-24"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, message=FALSE}
library(tidyverse)
library(tidymodels)
library(corrplot)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
```

### Exercise 1

```{r}
pok <- read.csv('./data/Pokemon.csv')
pok <- clean_names(pok)
pok2 <- subset(pok, type_1 == c('Bug','Fire','Grass',"Normal",'Water','Psychic'))
pok2$type_1 <- as.factor(pok2$type_1)
pok2$legendary <- as.factor(pok2$legendary)
```

```{r}
set.seed(619)

pok_split <- initial_split(pok2, prop = 0.80, strata = type_1)
pok_train <- training(pok_split)
pok_test <- testing(pok_split)
dim(pok_train)
dim(pok_test)
```

```{r}
pok_folds <- vfold_cv(pok_train, v = 5, strata = type_1)
pok_folds
```

```{r}
pok_rec <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def, data = pok_train) %>% 
  
  step_dummy(legendary, generation) %>% 
  step_normalize(all_numeric_predictors())
```

### Exercise 2


```{r}
pok_train %>% 
  select(is.numeric, -x) %>% 
  cor(use = 'complete.obs') %>% 
  corrplot(type = 'lower', diag = FALSE)

```
I removed the variable called 'x' from the correlation matrix. This variable contains an ID number for each pokemon, and it is not relevant in predictions.

The variable 'total' has a strong positive correlation with every variable except generation. This makes sense because total is a sum of all these other values. Defense and attack are positively correletated, which migh mean that stronger pokemon perform better in both of these areas. Defense speed (sp_def) and attack speed (sp_atk) are also positively correlated; if a pokemon is fast in one area it makes sense they would be fast in the other.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

```{r}
tree_spec <- decision_tree() %>% 
  set_engine('rpart')

class_tree_spec <- tree_spec %>% 
  set_mode('classification')

class_tree_wf <- workflow() %>% 
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>% 
  add_recipe(pok_rec)
```

```{r}
param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf,
  resamples = pok_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)
                           
```

```{r}
autoplot(tune_res)
```

The results for roc_auc are nearly constant until the cost complexity reached about 0.06, where the roc_auc slightly increased. As cost complexity increased beyond that value, roc_auc significantly decreased. In general, it seems like the single decision tree performed better with a smaller cost complexity penalty.

### Exercise 4

```{r}
collect_metrics(tune_res) %>% 
  arrange(desc(mean))

best_mod <- select_best(tune_res, metric = 'roc_auc')
```

The `roc_auc` of my best-performing pruned decision tree on the folds is 0.560

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
pok_final <- finalize_workflow(class_tree_wf, best_mod)
pok_final_fit <- fit(pok_final, data = pok_train)
pok_final_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()
```

### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

## For 231 Students

### Exercise 11

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?